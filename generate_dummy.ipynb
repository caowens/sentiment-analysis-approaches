{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Dummy Dataset to train a model on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After searching the internet, it has become clear that (understandably) I have had trouble finding a dataset of journal or diary entries with sentiment scores attached with the information that I would like to attach to it (like age of user, certain emotions, etc). So, I am going to generate some dummy data and fine-tune it to my liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Data Generation Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will want to generate some users. Certain demographics of users might give a better picture to my model about what the user's emotional state is. For now, we will just include the age. We can use the Faker library. \n",
    "\n",
    "The Faker library is a popular tool for generating fake data. We can use it to create names, addresses, dates, and even random text for journal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# Generate random names and dates for users\n",
    "num_users = 1000  # Number of users\n",
    "users = [{\"name\": fake.name(), \"birthdate\": fake.date_of_birth(minimum_age=18, maximum_age=70)} for _ in range(num_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Mrs. Sandra Jackson', 'birthdate': datetime.date(1977, 9, 14)}\n",
      "{'name': 'Robin Juarez', 'birthdate': datetime.date(1963, 7, 1)}\n",
      "{'name': 'Scott Sanchez', 'birthdate': datetime.date(1979, 10, 30)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(users[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating emotion-specific journal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of generating a sentence with OpenAI's [Completions API](https://platform.openai.com/docs/guides/gpt/completions-api). I start with the prompt \"Today, I felt\" and the API finishes the sentence. We will generated 1,000 sentences to use. But, first let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('OA_API_KEY')\n",
    "openai.api_key = api_key\n",
    "\n",
    "prompt = \"Today, I felt\"\n",
    "generated_text = openai.Completion.create(engine=\"text-davinci-002\", prompt=prompt, max_tokens=75)\n",
    "\n",
    "# Extract the generated text from the response\n",
    "journal_entry = generated_text.choices[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today, I felt extra anxious and had a panic attack.I was feeling really overwhelmed and stressed out about a number of things and it all just kind of hit me at once. I had to take a few deep breaths and try to relax. I eventually felt better, but it was a really tough day.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt + journal_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate some entries for our dataset.\n",
    "\n",
    "I want them to reflect the 6 basic emotions: sadness, happiness, fear, anger, surprise and disgust.\n",
    "To do this, I have to make sure there is an equal number of entries between each emotion. Let's make 300 entries per emotion. Let's also add \"bored\" as a neutral emotion.\n",
    "\n",
    "\n",
    "So, we will have 7 emotions, 300 entries each, for a total of 2,100 journal entries for our dataset. This should hopefully be enough to train a model on.\n",
    "\n",
    "We can do this by having the prompt \"Today, I felt\" and having ChatGPT finish the sentence based on the emotion given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['sad', 'happy', 'fear', 'angry', 'surprised', 'disgusted', 'bored']\n",
    "journal_entries = []\n",
    "\n",
    "prompt = \"Today, I felt \"\n",
    "for emotion in emotions:\n",
    "    emotive_prompt = prompt + emotion\n",
    "    for i in range(300):\n",
    "        generated_text = openai.Completion.create(engine=\"text-davinci-002\", prompt=emotive_prompt, max_tokens=75)\n",
    "        journal_entries.append(emotive_prompt + generated_text.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n",
      "318\n",
      "353\n",
      "300\n",
      "317\n",
      "307\n",
      "301\n",
      "304\n",
      "Today, I felt sad because\n",
      "\n",
      "I miss my friends and family.\n",
      "Today, I felt happy.\n",
      "\n",
      "I woke up this morning to the sun shining in through my window, and I just felt so happy. I don't know why, but I just felt really good today. I just felt like everything was going my way and that everything was going to be okay. I don't know what it is, but I just had this really good feeling all day.\n"
     ]
    }
   ],
   "source": [
    "print(len(journal_entries))\n",
    "print(len([1 for entry in journal_entries if 'sad' in entry]))\n",
    "print(len([1 for entry in journal_entries if 'happy' in entry]))\n",
    "print(len([1 for entry in journal_entries if 'fear' in entry]))\n",
    "print(len([1 for entry in journal_entries if 'angry' in entry]))\n",
    "print(len([1 for entry in journal_entries if 'surprised' in entry]))\n",
    "print(len([1 for entry in journal_entries if 'disgusted' in entry]))\n",
    "print(len([1 for entry in journal_entries if 'bored' in entry]))\n",
    "print(journal_entries[299])\n",
    "print(journal_entries[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try using sentiment lexicons to provide a positive or negative attribute to an entry. This takes the sum of positive or negative tokens in a sentence and assigns a positive or negative value based on the highest amount of either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afinn = Afinn()\n",
    "num_entries = 1000\n",
    "\n",
    "# Generate journal entries with positive sentiment\n",
    "positive_entries = [entry for entry in journal_entries if afinn.score(entry) > 0]\n",
    "\n",
    "# Generate journal entries with negative sentiment\n",
    "negative_entries = [entry for entry in journal_entries if afinn.score(entry) < 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "1295\n",
      "Today, I felt sad for no real reason.\n",
      "\n",
      "I think it might have just been the weather. It was raining and cloudy all day, which can definitely make someone feel down. Sometimes, when we don't know why we're feeling a certain way, it can help to think about what's going on around us that might be affecting our mood.\n",
      "Today, I felt sad.\n",
      "\n",
      "It could be because of many different reasons. Maybe something bad happened, or you miss someone. It's important to figure out what is making you sad, so you can try to fix the problem. Sometimes, though, you just need some time to yourself to feel better.\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_entries))\n",
    "print(len(negative_entries))\n",
    "print(positive_entries[5])\n",
    "print(negative_entries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to work well at all. Above, you can see that we have two obviously negative entries, however the first one is considered negative. This does not seem like a good way to evaluate sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's circle back to the entry generation. We initially used the prompt \"Today I felt \" ending with a certain emotion and had ChatGPT finish the entry.\n",
    "\n",
    "This seems to do what we asked, but there's an issue with this. Now every single journal entry will start with \"Today I felt\" and have the very emotion it's trying to predict in it. This could cause the model to expect \"Today, I felt\" to be in every entry and that for an entry to have a certain emotion, it would need to have that word's emotion in it.\n",
    "\n",
    "Let's change this so that the feeling of the emotion is still present in the entry, but the word and the original prompt isn't. We can do this by just telling chat gpt to make a journal entry as if it was the writer feeling a certain way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm feeling really happy today! I woke up this morning and the sun was shining, and I just felt really good. Everything feels like it's going my way and I'm just really enjoying life right now. I'm so grateful for everything I have and I just feel really lucky. I hope this feeling lasts forever!\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt to generate a journal entry with a specific emotion (e.g., disgust)\n",
    "prompt = \"Write a journal entry where the writer is feeling happy. Imagine you are the writer writing in your own personal diary.\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-002\",  # You can choose the engine based on your requirements\n",
    "    prompt=prompt,\n",
    "    max_tokens=75,  # Adjust the max tokens to control the length of the generated text\n",
    "    temperature=0.7,  # Adjust temperature for creativity (lower values make it more focused)\n",
    ")\n",
    "\n",
    "# Extract the generated text from the response\n",
    "generated_text = response.choices[0].text\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a much more realistic journal entry. Now, let's create the same amount of entries as before, where we had 300 entries of each of the 7 emotions we stated above, totaling to 2,100 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['sad', 'happy', 'fear', 'angry', 'surprised', 'disgusted', 'bored']\n",
    "entries = []\n",
    "\n",
    "prompt = \"Today, I felt \"\n",
    "for emotion in emotions:\n",
    "    prompt = \"Write a journal entry where the writer is feeling {}. Imagine you are the writer writing in your own personal diary.\".format(emotion)\n",
    "    for i in range(300):\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",  \n",
    "            prompt=prompt,\n",
    "            max_tokens=75,  \n",
    "            temperature=0.7, \n",
    "        )\n",
    "        generated_text = response.choices[0].text\n",
    "        entries.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI'm so angry right now. I can't believe that they would do that to me. I trusted them and they just stabbed me in the back. I don't know if I can ever forgive them for this.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries[900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame / File Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make our table. In the future, we will use the fake information we made earlier to create users, but for now, we just care about the entrys and their corresponding sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'disgusted', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored', 'bored']\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of times each word should be repeated\n",
    "word_repeat_count = 300\n",
    "\n",
    "# Generate the array\n",
    "corresponding_sentiment = [word for word in emotions for _ in range(word_repeat_count)]\n",
    "\n",
    "print(corresponding_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "corr_sent_indices = [i for i in range(len(emotions)) for _ in range(word_repeat_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I'm not sure...</td>\n",
       "      <td>sad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>sad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>sad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>sad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I'm not sure...</td>\n",
       "      <td>sad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Entry sentiment  sentiment_id\n",
       "0  \\n\\nI'm feeling really sad today. I'm not sure...       sad             0\n",
       "1  \\n\\nI'm feeling really sad today. I don't know...       sad             0\n",
       "2  \\n\\nI'm feeling really sad today. I don't know...       sad             0\n",
       "3  \\n\\nI'm feeling really sad today. I don't know...       sad             0\n",
       "4  \\n\\nI'm feeling really sad today. I'm not sure...       sad             0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Entry': entries,\n",
    "    'sentiment': corresponding_sentiment,\n",
    "    'sentiment_id': corr_sent_indices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dummy_journal_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, let's split it into training and test sets. For this, we'll do 80% for training, 20% for testing. For this to be accurate, we need to make sure the same amount from each category is taken from each. Since each emotion has 300 entries, we will take 60 entries from each emotion to make our testing set.\n",
    "\n",
    "In the end, we should have 240 of each emotion in the train set for a total of 1680,\n",
    "and 60 of each emotion in the test set for a total of 420."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680\n",
      "420\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "train, test = [], []\n",
    "start, mid, end = 0, 240, 300\n",
    "\n",
    "for i in range(len(emotions)):\n",
    "    train.extend(entries[start:mid]) \n",
    "    test.extend(entries[mid:end])\n",
    "    start += 300\n",
    "    mid += 300\n",
    "    end += 300\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(str(len(train) + len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_id_train = []\n",
    "for i in range(len(emotions)):\n",
    "    for j in range(240):\n",
    "        sent_id_train.append(i)\n",
    "\n",
    "sent_id_test = []\n",
    "for i in range(len(emotions)):\n",
    "    for j in range(60):\n",
    "        sent_id_test.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I'm not sure...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I'm not sure...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               entry  sentiment_id\n",
       "0  \\n\\nI'm feeling really sad today. I'm not sure...             0\n",
       "1  \\n\\nI'm feeling really sad today. I don't know...             0\n",
       "2  \\n\\nI'm feeling really sad today. I don't know...             0\n",
       "3  \\n\\nI'm feeling really sad today. I don't know...             0\n",
       "4  \\n\\nI'm feeling really sad today. I'm not sure...             0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = {\n",
    "    'entry': train,\n",
    "    'sentiment_id': sent_id_train\n",
    "}\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_journal_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nI'm feeling really sad toda...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nToday was a really tough day. I'm feeling ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nI'm feeling really sad toda...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nI'm feeling really sad today. I don't know...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               entry  sentiment_id\n",
       "0  \\n\\nDear Diary,\\n\\nI'm feeling really sad toda...             0\n",
       "1  \\n\\nToday was a really tough day. I'm feeling ...             0\n",
       "2  \\n\\nDear Diary,\\n\\nI'm feeling really sad toda...             0\n",
       "3  \\n\\nI'm feeling really sad today. I don't know...             0\n",
       "4  \\n\\nI'm feeling really sad today. I don't know...             0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = {\n",
    "    'entry': test,\n",
    "    'sentiment_id': sent_id_test\n",
    "}\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_journal_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a training and test set that we can train a model on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
