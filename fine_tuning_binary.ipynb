{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a binary dataset and fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#for pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#for BERT\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['positive','negative']\n",
    "entries = []\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    prompt = \"Write a journal entry where the writer is feeling {}. Imagine you are the writer writing in your own personal diary.\".format(sentiment)\n",
    "    for i in range(500):\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"gpt-3.5-turbo-instruct\",  \n",
    "            prompt=prompt,\n",
    "            max_tokens=250,  \n",
    "            temperature=0.7, \n",
    "        )\n",
    "        generated_text = response.choices[0].text\n",
    "        entries.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dear Diary,\n",
      "\n",
      "Today has been such a wonderful day, and I can't help but feel incredibly positive and grateful. The sun was shining brightly, and the birds were chirping, creating the perfect backdrop for my morning meditation. As I closed my eyes and focused on my breath, I felt a sense of peace and contentment wash over me. It was a great way to start my day.\n",
      "\n",
      "After my meditation, I decided to go for a walk in the park. The crisp air and the beautiful scenery were exactly what I needed to clear my mind and recharge my batteries. I also bumped into an old friend who I haven't seen in years. We had a lovely catch-up session, and it was heartwarming to see how much we have both grown and achieved since we last saw each other.\n",
      "\n",
      "As I continued my walk, I couldn't help but reflect on all the blessings in my life. I have a loving family, supportive friends, and a job that I am passionate about. Despite all the challenges and setbacks, I have faced, I am still standing strong and moving forward. I am grateful for all the lessons I have learned and the person I have become because of them.\n",
      "\n",
      "In the afternoon, I attended a yoga class,\n"
     ]
    }
   ],
   "source": [
    "print(entries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dear Diary,\n",
      "\n",
      "I am feeling so overwhelmed and negative today. It seems like everything is going wrong and I just can't catch a break. Every little thing that happens just adds to my already heavy load of negative emotions.\n",
      "\n",
      "I woke up this morning feeling exhausted and unmotivated. I didn't want to get out of bed, but I knew I had to face another day. As soon as I stepped out of the house, it started raining. Of course, I forgot my umbrella and got completely drenched. It's like the universe is conspiring against me.\n",
      "\n",
      "Work was a disaster. My boss was in a terrible mood and took it out on me. I tried my best to keep my head down and get my work done, but I couldn't shake off the feeling of being constantly watched and criticized. I feel like no matter how hard I work, it's never enough.\n",
      "\n",
      "And to top it all off, I got into an argument with my best friend. I don't even remember what it was about, but now we're not speaking to each other. It's like I'm losing the people closest to me and I have no control over it.\n",
      "\n",
      "I just feel so alone and defeated. It's like I'm stuck in\n"
     ]
    }
   ],
   "source": [
    "print(entries[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 1000 entries. 500 positive and 500 negative. Now let's make our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "labels = 500 * [1] # postive labels\n",
    "labels.extend(500 * [0]) # negative labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been a wonderful ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been a wonderful ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday, I am feeling incredi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>\\n\\nDear diary,\\n\\nToday has been a rough day....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been one of those...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been one of those...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been a difficult ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>\\n\\nDear diary,\\n\\nToday has been one of those...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 entry  label\n",
       "0    \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "1    \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "2    \\n\\nDear Diary,\\n\\nToday has been a wonderful ...      1\n",
       "3    \\n\\nDear Diary,\\n\\nToday has been a wonderful ...      1\n",
       "4    \\n\\nDear Diary,\\n\\nToday, I am feeling incredi...      1\n",
       "..                                                 ...    ...\n",
       "995  \\n\\nDear diary,\\n\\nToday has been a rough day....      0\n",
       "996  \\n\\nDear Diary,\\n\\nToday has been one of those...      0\n",
       "997  \\n\\nDear Diary,\\n\\nToday has been one of those...      0\n",
       "998  \\n\\nDear Diary,\\n\\nToday has been a difficult ...      0\n",
       "999  \\n\\nDear diary,\\n\\nToday has been one of those...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'entry': entries,\n",
    "    'label':labels\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nI am feeling extremely nega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>\\n\\nDear diary,\\n\\nToday has been a really tou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nI'm feeling so overwhelmed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been a rough day....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>\\n\\nDear diary,\\n\\nToday has been a wonderful ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been one of those...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>\\n\\nDear Diary, \\n\\nI can't seem to shake off ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 entry  label\n",
       "545  \\n\\nDear Diary,\\n\\nI am feeling extremely nega...      0\n",
       "718  \\n\\nDear diary,\\n\\nToday has been a really tou...      0\n",
       "604  \\n\\nDear Diary,\\n\\nI'm feeling so overwhelmed ...      0\n",
       "947  \\n\\nDear Diary,\\n\\nToday has been a rough day....      0\n",
       "492  \\n\\nDear diary,\\n\\nToday has been a wonderful ...      1\n",
       "..                                                 ...    ...\n",
       "734  \\n\\nDear Diary,\\n\\nToday has been one of those...      0\n",
       "420  \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "467  \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "840  \\n\\nDear Diary, \\n\\nI can't seem to shake off ...      0\n",
       "376  \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('binary_entries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nI am feeling extremely nega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nDear diary,\\n\\nToday has been a really tou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nI'm feeling so overwhelmed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been a rough day....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nDear diary,\\n\\nToday has been a wonderful ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been one of those...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>\\n\\nDear Diary, \\n\\nI can't seem to shake off ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>\\n\\nDear Diary,\\n\\nToday has been such a wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 entry  label\n",
       "0    \\n\\nDear Diary,\\n\\nI am feeling extremely nega...      0\n",
       "1    \\n\\nDear diary,\\n\\nToday has been a really tou...      0\n",
       "2    \\n\\nDear Diary,\\n\\nI'm feeling so overwhelmed ...      0\n",
       "3    \\n\\nDear Diary,\\n\\nToday has been a rough day....      0\n",
       "4    \\n\\nDear diary,\\n\\nToday has been a wonderful ...      1\n",
       "..                                                 ...    ...\n",
       "995  \\n\\nDear Diary,\\n\\nToday has been one of those...      0\n",
       "996  \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "997  \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "998  \\n\\nDear Diary, \\n\\nI can't seem to shake off ...      0\n",
       "999  \\n\\nDear Diary,\\n\\nToday has been such a wonde...      1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('binary_entries.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train, validation ad test data. We are taking the 70:15:15 ratio for this division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['entry'], df['label'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.3,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.5,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = temp_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained BERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding the padding length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the padding length as the maximum length of text found in the training texts, it might leave the training data sparse. Taking the least length would in turn lead to loss of information. Hence, we would plot the graph and see the “average” length and set it as the padding length to trade-off between the two extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,  11.,  31.,  54.,  84., 123., 115., 133., 101.,  46.]),\n",
       " array([196. , 199.1, 202.2, 205.3, 208.4, 211.5, 214.6, 217.7, 220.8,\n",
       "        223.9, 227. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lens = [len(i.split()) for i in train_text]\n",
    "plt.hist(train_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.27285714285713\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_lens)/len(train_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we take 215 as the padding length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pad_len = 215\n",
    "# tokenize and encode sequences\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "\ttrain_text.tolist(),\n",
    "\tmax_length = pad_len,\n",
    "\tpad_to_max_length = True,\n",
    "\ttruncation = True\n",
    ")\n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "\tval_text.tolist(),\n",
    "\tmax_length = pad_len,\n",
    "\tpad_to_max_length = True,\n",
    "\ttruncation = True\n",
    ")\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "\ttest_text.tolist(),\n",
    "\tmax_length = pad_len,\n",
    "\tpad_to_max_length = True,\n",
    "\ttruncation = True\n",
    ")\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 64\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first freeze the BERT pre-trained model, and then add layers as shown in the following code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the pretrained layers\n",
    "for param in bert.parameters():\n",
    "\tparam.requires_grad = False\n",
    "\n",
    "#defining new layers\n",
    "class BERT_architecture(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, bert):\n",
    "\t\t\n",
    "\t\tsuper(BERT_architecture, self).__init__()\n",
    "\t\t\n",
    "\t\tself.bert = bert\n",
    "\t\t\n",
    "        # dropout layer\n",
    "\t\tself.dropout = nn.Dropout(0.2)\n",
    "\t\t\n",
    "        # relu activation function\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\t\n",
    "        # dense layer 1\n",
    "\t\tself.fc1 = nn.Linear(768,512)\n",
    "\t\t\n",
    "        # dense layer 2 (Output layer)\n",
    "\t\tself.fc2 = nn.Linear(512,2)\n",
    "\t\t\n",
    "        # softmaz activation function\n",
    "\t\tself.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\t#define the forward pass\n",
    "\tdef forward(self, sent_id, mask):\n",
    "\t\t\n",
    "\t\t# pass the inputs to the model\n",
    "\t\t_, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "\t\t\n",
    "\t\tx = self.fc1(cls_hs)\n",
    "\t\t\n",
    "\t\tx = self.relu(x)\n",
    "\t\t\n",
    "\t\tx = self.dropout(x)\n",
    "\t\t\n",
    "\t\t# output layer\n",
    "\t\tx = self.fc2(x)\n",
    "\t\t\n",
    "\t\t# apply softmax activation\n",
    "\t\tx = self.softmax(x)\n",
    "\t\t\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_architecture(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, add an optimizer to enhance the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),lr = 1e-5) # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "\t\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "        \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "        \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(' Batch {:>5,} of {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad()\t\t\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "\t\n",
    "    print(\"\\nEvaluating...\")\n",
    "\t\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "        \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # # Calculate elapsed time in minutes.\n",
    "            # elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "            # Report progress.\n",
    "            print(' Batch {:>5,} of {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights are [1. 1.] for [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels \n",
    "                                     )\n",
    "print(\"class weights are {} for {}\".format(class_weights,np.unique(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    350\n",
       "0    350\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count of both the categories of training labels\n",
    "pd.value_counts(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrap class weights in tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push weights to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define loss function\n",
    "# add weights to handle the \"imbalance\" in the dataset\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6937006441029635\n",
      "Validation Loss: 0.685279111067454\n",
      "\n",
      " Epoch 2 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6821994673122059\n",
      "Validation Loss: 0.6742629408836365\n",
      "\n",
      " Epoch 3 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6711260676383972\n",
      "Validation Loss: 0.6650431950887045\n",
      "\n",
      " Epoch 4 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6659159660339355\n",
      "Validation Loss: 0.6574570536613464\n",
      "\n",
      " Epoch 5 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6552089832045815\n",
      "Validation Loss: 0.6523558100064596\n",
      "\n",
      " Epoch 6 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6486639055338773\n",
      "Validation Loss: 0.6454890171686808\n",
      "\n",
      " Epoch 7 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6388522169806741\n",
      "Validation Loss: 0.6389378507932028\n",
      "\n",
      " Epoch 8 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6359877152876421\n",
      "Validation Loss: 0.6315509080886841\n",
      "\n",
      " Epoch 9 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6332109841433439\n",
      "Validation Loss: 0.6254816055297852\n",
      "\n",
      " Epoch 10 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.6239600669253956\n",
      "Validation Loss: 0.6214003960291544\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print('\\nTraining Loss: {}'.format(train_loss))\n",
    "    print('Validation Loss: {}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86        75\n",
      "           1       0.81      0.97      0.88        75\n",
      "\n",
      "    accuracy                           0.87       150\n",
      "   macro avg       0.89      0.87      0.87       150\n",
      "weighted avg       0.89      0.87      0.87       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
